{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Partial Model Loading and Training\n",
                "\n",
                "This notebook demonstrates how to load a pretrained model into the first $x$ layers of a larger model and train the remaining layers (or fine-tune the whole thing)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from types import SimpleNamespace\n",
                "from trainer import DecoderOnlyTransformer, Trainer, get_optimizer, get_scheduler, get_dataloaders\n",
                "\n",
                "# Ensure we are in the right directory (if running from elsewhere)\n",
                "# os.chdir('c:\\\\Users\\\\mayda\\\\Desktop\\\\music-autocomplete') # Uncomment if needed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_partial_weights(model, checkpoint_path, n_layers_to_load, freeze=False):\n",
                "    \"\"\"\n",
                "    Loads weights from a checkpoint into the first `n_layers_to_load` blocks of the model.\n",
                "    Also loads embeddings.\n",
                "    \n",
                "    Args:\n",
                "        model: The target model (DecoderOnlyTransformer).\n",
                "        checkpoint_path: Path to the pretrained checkpoint.\n",
                "        n_layers_to_load: Number of blocks to load from the checkpoint.\n",
                "        freeze: If True, freezes the loaded parameters.\n",
                "    \"\"\"\n",
                "    if not os.path.exists(checkpoint_path):\n",
                "        print(f\"Checkpoint not found at {checkpoint_path}\")\n",
                "        return\n",
                "    \n",
                "    # Check if file is likely a Git LFS pointer\n",
                "    if os.path.getsize(checkpoint_path) < 1024:\n",
                "        print(f\"WARNING: Checkpoint file {checkpoint_path} is very small ({os.path.getsize(checkpoint_path)} bytes).\")\n",
                "        print(\"It might be a Git LFS pointer file. Please run 'git lfs pull' to download the actual model.\")\n",
                "        return\n",
                "        \n",
                "    print(f\"Loading weights from {checkpoint_path}...\")\n",
                "    try:\n",
                "        checkpoint = torch.load(checkpoint_path, map_location='cpu') # Load to CPU first\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading checkpoint (is it a valid PyTorch file?): {e}\")\n",
                "        return\n",
                "\n",
                "    state_dict = checkpoint['model_state_dict']\n",
                "    \n",
                "    # 1. Load Embeddings (Token & Positional)\n",
                "    # We assume vocab_size and block_size are compatible or we only load what matches if we were being very robust,\n",
                "    # but here we assume they match as per requirements.\n",
                "    try:\n",
                "        model.token_embedding.load_state_dict({'weight': state_dict['token_embedding.weight']})\n",
                "        model.position_embedding.load_state_dict({'weight': state_dict['position_embedding.weight']})\n",
                "        print(\"Loaded embeddings.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading embeddings: {e}\")\n",
                "\n",
                "    if freeze:\n",
                "        model.token_embedding.weight.requires_grad = False\n",
                "        model.position_embedding.weight.requires_grad = False\n",
                "    \n",
                "    # 2. Load Decoder Blocks\n",
                "    # The blocks are in model.blocks, which is a Sequential.\n",
                "    # State dict keys will look like 'blocks.0.sa.in_proj_weight', etc.\n",
                "    \n",
                "    loaded_blocks = 0\n",
                "    for i in range(n_layers_to_load):\n",
                "        if i >= len(model.blocks):\n",
                "            print(f\"Warning: Requested to load layer {i}, but model only has {len(model.blocks)} blocks. Stopping.\")\n",
                "            break\n",
                "            \n",
                "        block_prefix = f'blocks.{i}.'\n",
                "        \n",
                "        # Filter state_dict for this block\n",
                "        block_state = {}\n",
                "        for k, v in state_dict.items():\n",
                "            if k.startswith(block_prefix):\n",
                "                # Remove the prefix to match the block's internal state\n",
                "                local_key = k[len(block_prefix):]\n",
                "                block_state[local_key] = v\n",
                "        \n",
                "        if not block_state:\n",
                "            print(f\"Warning: No weights found for block {i} in checkpoint.\")\n",
                "            continue\n",
                "            \n",
                "        try:\n",
                "            model.blocks[i].load_state_dict(block_state)\n",
                "            loaded_blocks += 1\n",
                "            \n",
                "            if freeze:\n",
                "                for param in model.blocks[i].parameters():\n",
                "                    param.requires_grad = False\n",
                "                    \n",
                "        except Exception as e:\n",
                "            print(f\"Error loading block {i}: {e}\")\n",
                "            \n",
                "    print(f\"Successfully loaded {loaded_blocks} blocks.\")\n",
                "    if freeze:\n",
                "        print(f\"Frozen parameters for embeddings and first {loaded_blocks} blocks.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "# Adjust these parameters as needed\n",
                "\n",
                "class Config:\n",
                "    # Data\n",
                "    tokenized_dir = \"lmd_matched_processed\"\n",
                "    tokenizer_file = \"lmd_matched_tokenizer.json\"\n",
                "    num_songs = 1000\n",
                "    val_split = 0.1\n",
                "    \n",
                "    # Model Architecture (Target Model)\n",
                "    vocab_size = 5000\n",
                "    block_size = 1024\n",
                "    n_embed = 512\n",
                "    n_head = 16       # Target: 16 heads\n",
                "    n_blocks = 16     # Target: 16 blocks\n",
                "    dropout = 0.1\n",
                "    \n",
                "    # Training\n",
                "    learning_rate = 3e-4\n",
                "    max_epochs = 5\n",
                "    batch_size = 32\n",
                "    optimizer_type = 'adamw'\n",
                "    weight_decay = 0.01\n",
                "    adam_beta1 = 0.9\n",
                "    adam_beta2 = 0.95\n",
                "    momentum = 0.9\n",
                "    grad_clip = 1.0\n",
                "    \n",
                "    # Scheduler\n",
                "    scheduler = 'cosine'\n",
                "    min_learning_rate = 3e-5\n",
                "    lr_step_size = 10\n",
                "    lr_gamma = 0.1\n",
                "    lr_patience = 3\n",
                "    \n",
                "    # Checkpointing\n",
                "    checkpoint_path = 'trained_models/partial_run/model.pt'\n",
                "    load_checkpoint = False # We handle loading manually\n",
                "    run_name = 'partial_run_16x16'\n",
                "    compile = False\n",
                "    \n",
                "    # Partial Loading Settings\n",
                "    pretrained_path = 'trained_models/full_run_v2/model.pt' # Path to source model\n",
                "    n_layers_to_load = 8\n",
                "    freeze_loaded = True\n",
                "\n",
                "config = Config()\n",
                "\n",
                "# Ensure checkpoint directory exists\n",
                "os.makedirs(os.path.dirname(config.checkpoint_path), exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Initialize the Model\n",
                "model = DecoderOnlyTransformer(\n",
                "    vocab_size=config.vocab_size,\n",
                "    n_embed=config.n_embed,\n",
                "    n_head=config.n_head,\n",
                "    n_blocks=config.n_blocks,\n",
                "    block_size=config.block_size,\n",
                "    dropout=config.dropout\n",
                ")\n",
                "\n",
                "print(f\"Model created with {config.n_blocks} blocks.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load Partial Weights\n",
                "load_partial_weights(\n",
                "    model, \n",
                "    config.pretrained_path, \n",
                "    n_layers_to_load=config.n_layers_to_load, \n",
                "    freeze=config.freeze_loaded\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Setup Data, Optimizer, and Trainer\n",
                "train_loader, val_loader = get_dataloaders(config)\n",
                "total_steps = len(train_loader) * config.max_epochs\n",
                "\n",
                "optimizer = get_optimizer(model, config)\n",
                "scheduler = get_scheduler(optimizer, config, total_steps)\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    optimizer=optimizer,\n",
                "    scheduler=scheduler,\n",
                "    train_loader=train_loader,\n",
                "    val_loader=val_loader,\n",
                "    config=config\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Run Training\n",
                "trainer.train()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
